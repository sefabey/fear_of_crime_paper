---
title: "Lexicon of Crime and Police Words"
author: "Sefa Ozalp"
date: "26/07/2018"
output:
      html_document:
        keep_md: true
---
```{r}
knitr::opts_chunk$set(warning = F)
```



```{r}
library(tidyverse)
library(rhymer)
# install.packages('rhymer')
```

# Lexicon Paragraph
We compiled a list of 44 seeding words which includes words used when expressing fear of crime, different crime types and British slangs referring to police and criminal activity (find this list online [https://github.com/sefabey/fear_of_crime_paper/blob/master/data/FOC_seed_words_002.csv]). The seed list was used to create the lexicon by querying the Datamuse API -a word-finding engine which allows for querying rhyming words, similar spellings and semantically similar or contextually related words using multiple contraints such as synonyms, perfect and approximate rhymes, homophones, frequent followers, direct holonyms. Using the rhymer package (Landesberg 2017) in R, we queried Datamuse API for the first 100 words that 'means like' -i.e. words or sequence of words that are conceptually, semantically and lexically related to- the words in the seed list. After removing duplicates and manually removing clearly out of context results (such as query:coppers, Datamuse result:atomic number 29), we collated a lexicon consisting of 2538 words (find this file online [https://github.com/sefabey/fear_of_crime_paper/blob/master/data/FOC_lexicon_002_manual_edited.csv]). 

This lexicon was used to filter the 20m tweets in the dataset... (amir)

```{r}

seed_list <- read_csv("../data/FOC_seed_words.csv")
seed_list %>% distinct(words) #51 distinct words
```

```{r, warning=F}

query_datamuse <- function(x, ...){
    rhymer::get_means_like(word = x, ...)
}

crime_fear_lexicon <- seed_list %>% 
    select(words) %>%
    pull() %>%  
    map_df(query_datamuse, limit=100, .id = "id") %>% #5030 results
    mutate(id=as.integer(id)) %>% 
    distinct(word, .keep_all = T) %>% # drops to 3687
    left_join( select(seed_list, c(id, query_word=words)), by="id") %>% 
    select(id, query_word, everything()) %>% 
    mutate(tags=as.character(tags)) %>% 
    separate(tags, sep = ",", into = c("tag1", "tag2", "tag3", "tag4")) %>% 
    mutate_at(vars(tag1,tag2,tag3,tag4), .funs = function(x){
              str_extract(string=x, pattern =  regex("(?<=\")[[:alnum:]]+(?=\")"))})


# write_csv(crime_fear_lexicon,"../data/FOC_lexicon_001.csv")

# 
```
Not memoising above because no need as wrote to csv file once and using that.


Below removing words to remove identified manually. Again wrote once and commented oit write function.
```{r}
lexicon_filtered <- read_csv("../data/FOC_lexicon_001_manual.csv") %>% 
    filter(remove==0) # rows to be removed was labelled as 1, rest was 0

lexicon_filtered

# write_csv(lexicon_filtered, "../data/FOC_lexicon_001_manual_edited.csv")
```

## Take 2

New comments from the PI, need to do the following.

1) Remove fear related words from the seed list and lexicon
2) Revisit the final version of the lexicon and remove irrelevant words (such as drugs ~ prescription, charlatan)


### 2.1. Removing fear related words from the seed list and then from the lexicon.
```{r}
read_csv("../data/FOC_seed_words.csv") %>% 
    filter(context %in% c("fear")) # print fear related words

seed_list_002 <- read_csv("../data/FOC_seed_words.csv") %>% 
    filter(!context %in% c("fear")) %>% # glad that i added context var previously
    filter(!words=="shank") %>% # not that related
    mutate(id=seq.int(nrow(.)))

# write_csv(seed_list_002, "../data/FOC_seed_words_002.csv")
```

```{r}
lexicon_2 <- read_csv("../data/FOC_lexicon_001_manual_edited.csv") %>% 
    filter(!query_word %in% c("afraid","alone", "avoid", "fear", "scary", "worried")) %>% 
    select(-id) %>% 
    left_join(seed_list_002, by = c("query_word"="words")) %>% 
    select(id, everything(),-context, -explanation) # decrease to 

# write_csv(lexicon_2, "../data/FOC_lexicon_002.csv")
```

### 2.1. Revisit and remove irrelevant words

This is going to be done manually and outside R. 

Removing manually identified words

```{r}
lexicon_filtered_02 <- read_csv("../data/FOC_lexicon_002_manual.csv") %>% 
    filter(remove==0) # rows to be removed was labelled as 1, rest was 0

lexicon_filtered_02

write_csv(lexicon_filtered_02, "../data/FOC_lexicon_002_manual_edited.csv")
```


As words in English can take many forms, suffixes and prefixes, I'll add lemmas and stems of words in seperate columns. 

```{r}

lexicon_filtered_02 <- lexicon_filtered_02 %>% 
    mutate(lemmas=textstem::lemmatize_strings(word)) %>% 
    mutate(stems= textstem::stem_strings(word)) %>% 
    select(everything(),-remove, remove)

lexicon_filtered_02

write_csv(lexicon_filtered_02, "../data/FOC_lexicon_002_manual_edited.csv")
```

